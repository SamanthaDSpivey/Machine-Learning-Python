{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification using Neural Networks\n",
    " # Neural Networks usse 1+ hidden layer of multiple units to perform complex function approximation, which is said to have high model capacity.\n",
    "  # B/c of large # of hidden units, neural networks (nn) have many weights or parameters, which often leads to over-fitting & limits generalization.\n",
    "  # Finding optimal hyperparameter when fitting nn is essential for good performance.\n",
    "  # Another issue is computational complexity, requiring many optimization iterations, w/ each optimization iteration requiring update of a large # of parameters.\n",
    "\n",
    "# Import the packages to be used\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "# from statsmodels.api import datasets\n",
    "from sklearn import datasets      # Get dataset from sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 35)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data previously prepared in data preparation step.\n",
    "\n",
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 35)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "#Above- There are 1000 cases w/ 35 features & 1 label. Numeric features wer Zscore scaled so they are 0 centered (mean removed) & unit variance (divide by std dev).\n",
    "\n",
    "#Below- Neural network is known to be problematic when there is a significant class imbalance\n",
    " # Unfortunately, nn have no method for weighting classes. Alternatives:\n",
    " # 1) Impute new values using statistical alogrithm.\n",
    " # 2) Undersample majority of cases. For this method, a # of cases = to minority case are Bernoulli sampled from majority case.\n",
    " # 3) Oversample minority cases. For this method, a # of minority cases are resampled until = # of majority cases.\n",
    "    \n",
    "# Oversample the minority cases (bad credit customers).\n",
    " # Create data set w/ balanced cases.\n",
    "    \n",
    "temp_Labels = Labels[Labels == 1] \n",
    "temp_Features = Features[Labels == 1,:]\n",
    "temp_Features = np.concatenate((Features, temp_Features), axis = 0)\n",
    "temp_Labels = np.concatenate((Labels, temp_Labels), axis = 0) \n",
    "\n",
    "print(temp_Features.shape)\n",
    "print(temp_Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above- Now w/ 1300 cases overall.\n",
    "\n",
    "#Below- Perform model selection w/ nested cross validation (ncv) for optimal hyperparameters & model selection.\n",
    " # Compute inner loop to find optimal learning rate parameter w/ 3 fold cv since training nn is computationally intensive.\n",
    "   # Additional folds would give better estimates but at cost of greater computation time.\n",
    "\n",
    "#Define inside & outer folds\n",
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=3, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "False\n",
      "0.8\n",
      "0.999\n"
     ]
    }
   ],
   "source": [
    "# Estimate optimal hyperparameters using 3 fold cv.\n",
    " # In interest of computational efficiency, values for only 4 parameters will be searched.\n",
    "  #1) Grid of 4 hyperparameters:\n",
    "  #   A) alpha- is l2 regularization hyperparameter\n",
    "  #   B) early_stopping- determine when training metric becomes worse following an iteration of optimization algorithm stop training at previous iteration.\n",
    "  #       This is powerful method to prevent over-fitting or ML models in general & nn in particular.\n",
    "  #   C) beta_1 & beta_2- hyperparameters that control adaptive learning rate used by Adam optimizer.\n",
    "  #2) Model is fit on grid\n",
    "  #3) Best estimated hyperparameters are displayed\n",
    "\n",
    "# Code below searches over a 3x3x3x2 or 54 element grid using 3 fold cv, requiring model to be trained 162 times.\n",
    " # Execution will take some time.\n",
    "    \n",
    "\n",
    "param_grid = {\"alpha\":[0.0000001,0.000001,0.00001],   # Define dictionary for grid search & model object to search on\n",
    "              \"early_stopping\":[True, False], \n",
    "              \"beta_1\":[0.95,0.90,0.80], \n",
    "              \"beta_2\":[0.999,0.9,0.8]}\n",
    "\n",
    "nn_clf = MLPClassifier(hidden_layer_sizes = (100,100),    # Define Neural Network model\n",
    "                       max_iter=300)\n",
    "\n",
    "nr.seed(3456)\n",
    "nn_clf = ms.GridSearchCV(estimator = nn_clf, param_grid = param_grid,   # Perform grid search over parameters\n",
    "                      cv = inside,                            # Use inside folds\n",
    "                      scoring = 'recall',\n",
    "                      return_train_score = True)\n",
    "\n",
    "nr.seed(6677)\n",
    "nn_clf.fit(temp_Features, temp_Labels)\n",
    "print(nn_clf.best_estimator_.alpha)\n",
    "print(nn_clf.best_estimator_.early_stopping)\n",
    "print(nn_clf.best_estimator_.beta_1)\n",
    "print(nn_clf.best_estimator_.beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Performance Metric = 0.866\n",
      "SDT of the Metric       = 0.006\n",
      "Outcomes by CV Fold\n",
      "Fold  1    0.865\n",
      "Fold  2    0.874\n",
      "Fold  3    0.859\n"
     ]
    }
   ],
   "source": [
    "#Above- Estimated optimal learning rate parameters are alpha=1e-05, early_stopping=False, beta_1=0.8, beta_2=0.999.\n",
    "\n",
    "#Below- Perform outer cv of mode, which will take time.\n",
    "\n",
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(nn_clf, temp_Features, temp_Labels, cv = outside)     # Use outside folds\n",
    "                                 \n",
    "\n",
    "print('Mean Performance Metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the Metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by CV Fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above- Std dev of mean of Recall is an order of magnitude less than mean itself, indicating that this model is likely to generalize well, but level of performance is unclear.\n",
    "\n",
    "#Below- Build, train & evaluate model w/ balanced cases & estimated optimal hyperparameters.\n",
    "  # Create Bernoulli sampled test & training subsets\n",
    "  # Oversample minority case for training data subset\n",
    "\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])    # Randomly sample cases to create independent training & test data\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "x_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "x_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])\n",
    "\n",
    "y_temp = y_train[y_train == 1]     # Oversample minority case for training data\n",
    "x_temp = x_train[y_train == 1,:]\n",
    "x_train = np.concatenate((x_train, x_temp), axis = 0)\n",
    "y_train = np.concatenate((y_train, y_temp), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.8,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define neural network model object w/ optimal hyperparmeters & fits model to training data subset\n",
    "\n",
    "nr.seed(1115)\n",
    "nn_mod = MLPClassifier(hidden_layer_sizes = (100,100), \n",
    "                       alpha = nn_clf.best_estimator_.alpha, \n",
    "                       early_stopping = nn_clf.best_estimator_.early_stopping, \n",
    "                       beta_1 = nn_clf.best_estimator_.beta_1, \n",
    "                       beta_2 = nn_clf.best_estimator_.beta_2,\n",
    "                       max_iter = 300)\n",
    "nn_mod.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion Matrix\n",
      "                 Score Positive    Score Negative\n",
      "Actual Positive       169                43\n",
      "Actual Negative        43                45\n",
      "\n",
      "Accuracy        0.71\n",
      "AUC             0.72\n",
      "Macro Precision 0.65\n",
      "Macro Recall    0.65\n",
      " \n",
      "           Positive      Negative\n",
      "Num Case      212            88\n",
      "Precision    0.80          0.51\n",
      "Recall       0.80          0.51\n",
      "F1           0.80          0.51\n"
     ]
    }
   ],
   "source": [
    "#Above- Hyperparameters of nn model object are as expected. \n",
    "\n",
    "#Below- Score & evaluate the test model\n",
    "\n",
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion Matrix')\n",
    "    print('                 Score Positive    Score Negative')\n",
    "    print('Actual Positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual Negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro Precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro Recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num Case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = nn_mod.predict_proba(x_test)\n",
    "print_metrics(y_test, probabilities, 0.5)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above- Performance of the nn above is less than ideal and is worse than if only beta_1 & beta_2 were searched.\n",
    "  # Negative (bad credit) case recall is adequate, but precision is poor.\n",
    "  # Perhaps oversampling does not help.\n",
    "\n",
    "## Summary- \n",
    "  # Used 3 fold to find estimated optimal hyperparameters for neural network (nn) model to classify credit risk cases.\n",
    "    # Oversampling of minority case for training data was required to deal w/ class imbalance, & result achieved were marginal at best.\n",
    "    # Perhaps model w/ greater capacity would achieve better results, or different approach to deal w/ class imbalance would be more successful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
