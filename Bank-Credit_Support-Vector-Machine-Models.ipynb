{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machine Models (SVMs) attempts to maximally separate classes by finding support vector w/ lowest error rate or maximum separation.\n",
    "  # Can use many types of kernal functions, but most common are linear & radial basis function (rbf).\n",
    "     # Linear basis function (lbf) attempts to separate classes by finding hyperplanes in feature space that maximally separate classes.\n",
    "     # Radial basis function (rbf) uses set of local Guassian shaped basis kernels to find nonlinear separation of classes.\n",
    "  # SVMs are widely use & powerful category of ML algorithms w/ many variations.\n",
    "\n",
    "#Import the packages to be used\n",
    "\n",
    "from sklearn import svm, preprocessing\n",
    "#from statsmodels.api import datasets\n",
    "from sklearn import datasets ## Get dataset from sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 35)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data previously prepared in data preparation step.\n",
    "\n",
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above- There are 1000 cases w/ 35 features & 1 label. Numeric features wer Zscore scaled so they are 0 centered (mean removed) & unit variance (divide by std dev).\n",
    "\n",
    "#Below- Perform model selection w/ nested cross validation (ncv) for optimal hyperparameters & model selection.\n",
    " # Compute inner loop to find optimal learning rate parameter w/ 5 fold cv since training SVM is computationally intensive.\n",
    "   # Additional folds would give better estimates but at cost of greater computation time.\n",
    "\n",
    "#Define inside & outer folds\n",
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=5, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "# Estimate optimal hyperparameters using 5 fold cv.\n",
    " # In interest of computational efficiency, values for only 2 parameters will be searched.\n",
    "  #1) Grid of 2 hyperparameters:\n",
    "  #   A) C- is inverse of lamda of l2 regularization hyperparameter\n",
    "  #   B) gamma- span of RBF kernel\n",
    "  #2) Class weights are used due to class imbalance & difference in cost to bank of misclassification of bad credit risk customers\n",
    "  #3) Model is fit on grid & best estimated hyperparameters are displayed\n",
    "\n",
    "nr.seed(3456)\n",
    "\n",
    "param_grid = {\"C\": [1, 10, 100, 1000], \"gamma\":[1.0/50.0, 1.0/200.0, 1.0/500.0, 1.0/1000.0]}   # Define dictionary for grid search & model object to search on\n",
    "svc_clf = svm.SVC(class_weight = {0:0.33, 1:0.67})    # Define SVM model\n",
    "\n",
    "clf = ms.GridSearchCV(estimator = svc_clf, param_grid = param_grid,    # Perform grid search over parameters\n",
    "                      cv = inside,                       # Use the inside folds\n",
    "                      scoring = 'roc_auc',\n",
    "                      return_train_score = True)\n",
    "clf.fit(Features, Labels)\n",
    "print(clf.best_estimator_.C)\n",
    "print(clf.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Performance Metric = 0.774\n",
      "SDT of the Metric       = 0.016\n",
      "Outcomes by CV Fold\n",
      "Fold  1    0.761\n",
      "Fold  2    0.758\n",
      "Fold  3    0.800\n",
      "Fold  4    0.768\n",
      "Fold  5    0.782\n"
     ]
    }
   ],
   "source": [
    "#Above- Estimated optimal learning rate parameters are C=10, & gamma=0.02.\n",
    "\n",
    "#Below- Perform outer cv of model.\n",
    "\n",
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(clf, Features, Labels, \n",
    "                                 cv = outside)       # Use the outside folds\n",
    "\n",
    "print('Mean Performance Metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the Metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by CV Fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above- Std dev of mean of AUC is more than an order of magnitude smaller than mean itself, indicating that this model is likely to generalize well, but level of performance is unclear.\n",
    "\n",
    "#Below- Build, train & evaluate model w/ estimated optimal hyperparameters.\n",
    "  # Create Bernoulli sampled test & training subsets\n",
    "\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])    # Randomly sample cases to create independent training & test data\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "x_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "x_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight={0: 0.33, 1: 0.67}, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.02, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define rbf SVM model object w/ optimal hyperparmeters & fit model to training data subset\n",
    "\n",
    "nr.seed(1115)\n",
    "svm_mod = svm.SVC(C = clf.best_estimator_.C,\n",
    "                  gamma = clf.best_estimator_.gamma,\n",
    "                  class_weight = {0:0.33, 1:0.67},\n",
    "                  probability=True) \n",
    "svm_mod.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion Matrix\n",
      "                 Score Positive    Score Negative\n",
      "Actual Positive       187                25\n",
      "Actual Negative        48                40\n",
      "\n",
      "Accuracy        0.76\n",
      "AUC             0.80\n",
      "Macro Precision 0.71\n",
      "Macro Recall    0.67\n",
      " \n",
      "           Positive      Negative\n",
      "Num Case      212            88\n",
      "Precision    0.80          0.62\n",
      "Recall       0.88          0.45\n",
      "F1           0.84          0.52\n"
     ]
    }
   ],
   "source": [
    "#Above- Hyperparameters of rbf SVM model object are as expected. \n",
    "\n",
    "#Below- Score & evaluate the test model\n",
    "\n",
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion Matrix')\n",
    "    print('                 Score Positive    Score Negative')\n",
    "    print('Actual Positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual Negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro Precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro Recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num Case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = svm_mod.predict_proba(x_test)\n",
    "print_metrics(y_test, probabilities, 0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above- Performance of the rbf SVM above is acceptable.\n",
    "  # Large majority of high risk customers are identified, but at cost of large # of FP & low precision for negative cases.\n",
    "  # AUC= 0.80 is better than mean achieved w/ 5 fold cv.\n",
    "\n",
    "## Summary- \n",
    "  # Used 5 fold to find estimated optimal hyperparameters for nonlinear SVM model to classify credit risk cases.\n",
    "    # Model appears to generalize well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
